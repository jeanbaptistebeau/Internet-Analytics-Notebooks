{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *B*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Amaury Combes*\n",
    "* *Vincenzo Bazzucchi*\n",
    "* *Alexis Montavon*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import scipy.sparse\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'courseId': 'MSE-440',\n",
       " 'description': \"The latest developments in processing and the novel generations of organic composites are discussed. Nanocomposites, adaptive composites and biocomposites are presented. Product development, cost analysis and study of new markets are practiced in team work. Content Basics of composite materialsConstituentsProcessing of compositesDesign of composite structures\\xa0Current developmentNanocomposites Textile compositesBiocompositesAdaptive composites\\xa0ApplicationsDriving forces and marketsCost analysisAerospaceAutomotiveSport Keywords Composites - Applications - Nanocomposites - Biocomposites - Adaptive composites - Design - Cost Learning Prerequisites Required courses Notion of polymers Recommended courses Polymer Composites Learning Outcomes By the end of the course, the student must be able to: Propose suitable design, production and performance criteria for the production of a composite partApply the basic equations for process and mechanical properties modelling for composite materialsDiscuss the main types of composite applications Transversal skills Use a work methodology appropriate to the task.Use both general and domain specific IT resources and toolsCommunicate effectively with professionals from other disciplines.Evaluate one's own performance in the team, receive and respond appropriately to feedback. Teaching methods Ex cathedra and invited speakers Group sessions with exercises or work on the project Expected student activities Attendance at lectures Design of a composite part, bibliography search \\xa0 Assessment methods Written exam report and oral presentation in class\",\n",
       " 'name': 'Composites technology'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not running on your machine, please use the nltk.download() method to download the WorndNet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing part, we decided to use the stop-words you gave us to remove them because they are too frequent. We made a list of the punctuation we could find in the data and removed it as it's not briging any information and we used the nltk lemmatizer to group together different forms of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class Cleaner:\n",
    "\n",
    "    def __init__(self, punctuation, stop_words, lemmatizer=None, stemmer=None):\n",
    "        self._punctuation = list(punctuation)\n",
    "        self._lemmatizer = lemmatizer\n",
    "        self._stemmer = stemmer\n",
    "        self._stop_words = list(stop_words)\n",
    "\n",
    "    def cleanWord(self, word):\n",
    "        rWord = \"\"\n",
    "        for l in word.lower():\n",
    "            if not(l in self._punctuation) and l.isalpha():\n",
    "                rWord += l\n",
    "            else:\n",
    "                rWord += ' '\n",
    "\n",
    "        if self._lemmatizer is not None:\n",
    "            rWord = self._lemmatizer.lemmatize(rWord)\n",
    "        if self._stemmer is not None:\n",
    "            rWord = self._stemmer.stem(rWord)\n",
    "        if rWord in self._stop_words:\n",
    "            return ''\n",
    "        else:\n",
    "            return rWord + \" \"\n",
    "\n",
    "    def cleanMessage(self, message):\n",
    "        separateAtUpper = re.sub('([A-Z]{1})', r' \\1', message)\n",
    "        words = separateAtUpper.split()\n",
    "        rMessage = \"\"\n",
    "        for w in words:\n",
    "            tmp = self.cleanWord(w)\n",
    "            if len(w) > 1:\n",
    "                rMessage += tmp\n",
    "        return rMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ponctuation = [',', '.', '-', ':', '\\xa0', '%', '_', '!', '?', ';', '(', ')', '\\n']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaner = Cleaner(ponctuation, stopwords, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "for course in courses:\n",
    "    cleaned.append({\n",
    "            'courseId': course['courseId'],\n",
    "            'description': cleaner.cleanMessage(course['description']),\n",
    "            'name': course['name']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove very frequent words\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for course in cleaned:\n",
    "    for w in course['description'].split(' '):\n",
    "        if w != '':\n",
    "            c[w] += 1\n",
    "counted = sorted([(w, c[w]) for w in c], key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 1929),\n",
       " ('method', 1644),\n",
       " ('learning', 1482),\n",
       " ('design', 935),\n",
       " ('content', 917),\n",
       " ('system', 868),\n",
       " ('course', 811),\n",
       " ('analysis', 754),\n",
       " ('project', 738),\n",
       " ('model', 731),\n",
       " ('basic', 699),\n",
       " ('end', 675),\n",
       " ('assessment', 652),\n",
       " ('s', 647),\n",
       " ('to', 644),\n",
       " ('outcome', 626),\n",
       " ('concept', 622),\n",
       " ('data', 607),\n",
       " ('teaching', 597),\n",
       " ('prerequisite', 597),\n",
       " ('keywords', 573),\n",
       " ('work', 554),\n",
       " ('skill', 550),\n",
       " ('lecture', 526),\n",
       " ('introduction', 508),\n",
       " ('activity', 506),\n",
       " ('theory', 504),\n",
       " ('exercise', 471),\n",
       " ('problem', 458),\n",
       " ('application', 453),\n",
       " ('presentation', 446),\n",
       " ('exam', 442),\n",
       " ('report', 438),\n",
       " ('energy', 433),\n",
       " ('material', 428),\n",
       " ('expected', 426),\n",
       " ('process', 419),\n",
       " ('evaluate', 406),\n",
       " ('plan', 404),\n",
       " ('time', 402),\n",
       " ('transversal', 397),\n",
       " ('required', 394),\n",
       " ('recommended', 392),\n",
       " ('based', 386),\n",
       " ('research', 379),\n",
       " ('engineering', 375),\n",
       " ('structure', 375),\n",
       " ('information', 363),\n",
       " ('class', 355),\n",
       " ('written', 352)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "too_frequent = set([w for w, c in counted[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_courses = []\n",
    "for course in cleaned:\n",
    "    new_words = ''\n",
    "    for word in course['description'].split(' '):\n",
    "        if word != '' and word not in too_frequent:\n",
    "            new_words = new_words + ' ' + word\n",
    "    clean_courses.append({\n",
    "        'courseId': course['courseId'],\n",
    "        'description': new_words,\n",
    "        'name': course['name']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fetch the index of Internet Analytics in the documents\n",
    "ix_id = 0\n",
    "for course in clean_courses:\n",
    "    if course['courseId'] == 'COM-308':\n",
    "        ix_id = clean_courses.index(course)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_words = []\n",
    "for word in sorted(clean_courses[ix_id]['description'].split(' ')):\n",
    "    if word not in ix_words and word != '':\n",
    "        ix_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the the terms in the pre-processed description of the IX class in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acquired', 'ad', 'advertisement', 'algebra', 'algorithm', 'algorithms', 'analytics', 'analyze', 'apache', 'auction', 'auctions', 'balance', 'cathedra', 'chains', 'cloud', 'clustering', 'collection', 'combination', 'commerce', 'communication', 'community', 'computing', 'concepts', 'concrete', 'contained', 'coverage', 'curated', 'current', 'datasets', 'decade', 'dedicated', 'designed', 'detection', 'develop', 'dimensionality', 'draw', 'e', 'effectiveness', 'efficiency', 'etc', 'explore', 'explores', 'fields', 'final', 'foundational', 'framework', 'function', 'fundamental', 'good', 'graph', 'graphs', 'hadoop', 'hands', 'homework', 'important', 'infrastructure', 'inspired', 'internet', 'java', 'key', 'knowledge', 'lab', 'laboratory', 'large', 'lectures', 'linear', 'm', 'machine', 'main', 'map', 'markov', 'media', 'midterm', 'mining', 'modeling', 'models', 'modelsdata', 'networking', 'networks', 'number', 'on', 'online', 'past', 'practical', 'practice', 'provide', 'question', 'real', 'recommender', 'reduce', 'reduction', 'related', 'retrieval', 'scale', 'search', 'seek', 'self', 'service', 'services', 'session', 'sessions', 'social', 'spark', 'specifically', 'start', 'statistics', 'stochastic', 'stream', 'systems', 'technique', 'theoretical', 'this', 'together', 'topic', 'typical', 'ubiquitous', 'user', 'weekly', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(ix_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_words():\n",
    "    all_words = []\n",
    "    for course in clean_courses:\n",
    "        for word in course['description'].split(' '):\n",
    "            if word not in all_words and word != '':\n",
    "                all_words.append(word)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_TD_matrix(terms, documents):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    data = []\n",
    "    terms_inv = {t: i for i, t in enumerate(terms)}\n",
    "    for doc_index, doc in enumerate(documents):\n",
    "        for word in doc['description'].split(' '):\n",
    "            if word != '':\n",
    "                columns.append(doc_index)\n",
    "                rows.append(terms_inv[word])\n",
    "                data.append(1)\n",
    "    return csr_matrix((data, (rows, columns)), shape=(len(terms), len(documents)), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_TF(word_index, doc_index, M):\n",
    "    \"\"\"\n",
    "    Computes the term frequency of term with index word_index in document with index doc_index\n",
    "    using the data in the term document matrix M\n",
    "    \"\"\"\n",
    "    doc_col = M.getcol(doc_index)\n",
    "    return doc_col[word_index].toarray()[0, 0] / doc_col.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_IDF(word_index, M):\n",
    "    \"\"\"\n",
    "    Computes the inverse document frequency of the term with index word_index using\n",
    "    the data in the therm document matrix M\n",
    "    \"\"\"\n",
    "    ni = M.getrow(word_index).count_nonzero()\n",
    "    return -math.log(ni / M.shape[1] , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = get_all_words()\n",
    "TD = create_TD_matrix(terms, clean_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_all_words = set(clean_courses[ix_id]['description'].split(' '))\n",
    "ix_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word in ix_all_words:\n",
    "    if word != '':\n",
    "        w_id = terms.index(word)\n",
    "        x = compute_TF(w_id, ix_id, TD)\n",
    "        y = compute_IDF(w_id, TD)\n",
    "        ix_scores.update({word : x*y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mining : 5.3904738076963925\n",
      "online : 4.983204757457022\n",
      "social : 4.568167258178178\n",
      "world : 4.153129758899334\n",
      "explore : 3.7904738076963924\n",
      "networking : 3.767196384589916\n",
      "hadoop : 3.4952369038481965\n",
      "real : 3.295148763771762\n",
      "service : 3.294098847706143\n",
      "recommender : 3.261251903559734\n",
      "commerce : 3.0952369038481966\n",
      "ad : 2.9664656658932516\n",
      "retrieval : 2.861251903559734\n",
      "datasets : 2.7722949350251547\n",
      "internet : 2.6952369038481963\n"
     ]
    }
   ],
   "source": [
    "sorted_scores = sorted(ix_scores, key=lambda x: ix_scores[x], reverse=True)\n",
    "for k in sorted_scores[:15]:\n",
    "    print(\"{} : {}\".format(k, ix_scores[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the 15 terms with the highest TF-IDF score from the IX course description. A large TF-IDF score means that the term is rare overall documents but very prominent in this specific one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(doc1, doc2_id, M):\n",
    "    doc2 = M.getcol(doc2_id)\n",
    "    num = doc1.transpose().dot(doc2)\n",
    "    denom = np.sqrt(doc1.power(2).sum()) * np.sqrt(doc2.power(2).sum())\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "facebook = csr_matrix(([1], ([terms.index('facebook')], [0])), shape=(len(terms), 1), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markov_chains = csr_matrix(([1, 1], ([terms.index('markov'), terms.index('chain')], [0, 0])), shape=(len(terms), 1), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(854, 854)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_fb = {}\n",
    "for i in range(len(clean_courses)):\n",
    "    similarities_fb.update({i : compute_sim(facebook, i, TD)})\n",
    "    \n",
    "similarities_mc = {}\n",
    "for j in range(len(clean_courses)):\n",
    "    similarities_mc.update({j : compute_sim(markov_chains, j, TD)})\n",
    "\n",
    "len(similarities_fb), len(similarities_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational Social Media :   (0, 0)\t0.103142124626\n",
      "Composites technology : \n",
      "Image Processing for Life Science : \n",
      "Global business environment : \n",
      "Electrochemical nano-bio-sensing and bio/CMOS interfaces : \n"
     ]
    }
   ],
   "source": [
    "sorted_sim_fb = sorted(similarities_fb, key=lambda x: similarities_fb[x], reverse=True)\n",
    "for k in sorted_sim_fb[:5]:\n",
    "    print(\"{} : {}\".format(clean_courses[k]['name'], similarities_fb[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that only one course have some similarities with the query 'Facebook' and this is because it is the only course containing this term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied stochastic processes :   (0, 0)\t0.542609516234\n",
      "Applied probability & stochastic processes :   (0, 0)\t0.437927872963\n",
      "Markov chains and algorithmic applications :   (0, 0)\t0.352864873113\n",
      "Supply chain management :   (0, 0)\t0.3478327965\n",
      "Statistical Sequence Processing :   (0, 0)\t0.243733339111\n"
     ]
    }
   ],
   "source": [
    "sorted_sim_mc = sorted(similarities_mc, key=lambda x: similarities_mc[x], reverse=True)\n",
    "for k in sorted_sim_mc[:5]:\n",
    "    print(\"{} : {}\".format(clean_courses[k]['name'], similarities_mc[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that at least 5 courses have similarities with the query 'Markov chains' as multiple courses contains at least one of those term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the TFIDF matrix and stores it together with the clean_courses and terms in order to be able to use them in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows, cols, vals = scipy.sparse.find(TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDF = np.array([compute_IDF(i, TD) for i in range(len(terms))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.zeros(len(vals))\n",
    "\n",
    "for i in range(len(vals)):\n",
    "    data[i] = compute_TF(rows[i], cols[i], TD) * IDF[rows[i]]\n",
    "\n",
    "TFIDF = csr_matrix((data, (rows, cols)), shape=TD.shape, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove â€˜term-document-matrix.npzâ€™: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm tfidf.npz\n",
    "np.savez('tfidf', data = TFIDF.data ,indices=TFIDF.indices,\n",
    "             indptr=TFIDF.indptr, shape=TFIDF.shape)\n",
    "\n",
    "!rm term-document-matrix.npz\n",
    "np.savez('term-document-matrix', data = TD.data ,indices=TD.indices,\n",
    "             indptr=TD.indptr, shape=TD.shape)\n",
    "\n",
    "!rm terms.txt\n",
    "with open('terms.txt', 'w') as f:\n",
    "    for term in terms:\n",
    "        f.write(term + '\\n')\n",
    "\n",
    "import json\n",
    "!rm courses.txt\n",
    "with open('courses.txt', 'w') as f:\n",
    "    for course in clean_courses:\n",
    "        f.write(course['name'] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
